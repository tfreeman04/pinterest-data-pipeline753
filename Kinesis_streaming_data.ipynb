{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8def82bd-953d-4d67-8def-bd2f5aa1bedb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# URL processing\n",
    "import urllib\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KinesisDataRead\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Get the AWS access key and secret key from the Spark DataFrame\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').first()['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').first()['Secret access key']\n",
    "\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "# Set the AWS credentials in Spark context\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY)\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY)\n",
    "\n",
    "# Define AWS region and Kinesis stream details\n",
    "kinesis_region = \"us-east-1\"  \n",
    "kinesis_stream_name_pin = \"streaming-0eaa2e755d1f-pin\"\n",
    "kinesis_stream_name_geo = \"streaming-0eaa2e755d1f-geo\"\n",
    "kinesis_stream_name_user = \"streaming-0eaa2e755d1f-user\"\n",
    "\n",
    "# Read data from Kinesis stream \n",
    "pin_kinesis_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", kinesis_stream_name_pin) \\\n",
    "    .option(\"region\", kinesis_region) \\\n",
    "    .option(\"awsAccessKey\", ACCESS_KEY) \\\n",
    "    .option(\"awsSecretKey\", SECRET_KEY) \\\n",
    "    .load()\n",
    "    \n",
    "\n",
    "# Automatically infer schema and parse JSON data\n",
    "pin_df = pin_kinesis_df \\\n",
    "    .selectExpr(\"CAST(data AS STRING) as json_data\")\n",
    "\n",
    "# Write the streaming data to the console (for testing)\n",
    "pin_query = pin_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination\n",
    "pin_query.awaitTermination()\n",
    "\n",
    "# Read data from kinesis stream \n",
    "geo_kinesis_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", kinesis_stream_name_geo) \\\n",
    "    .option(\"region\", kinesis_region) \\\n",
    "    .option(\"awsAccessKey\", ACCESS_KEY) \\\n",
    "    .option(\"awsSecretKey\", SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "geo_df = geo_kinesis_df\\\n",
    "    .selectExpr(\"CAST(data AS STRING) as json_data\")\n",
    "\n",
    "# Write the streaming data to the console (for testing)\n",
    "geo_query = geo_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination\n",
    "geo_query.awaitTermination()\n",
    "\n",
    "# Read data from kinesis stream \n",
    "user_kinesis_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", kinesis_stream_name_user) \\\n",
    "    .option(\"region\", kinesis_region) \\\n",
    "    .option(\"awsAccessKey\", ACCESS_KEY) \\\n",
    "    .option(\"awsSecretKey\", SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "user_df = user_kinesis_df\\\n",
    "    .selectExpr(\"CAST(data AS STRING) as json_data\")\n",
    "\n",
    "# Write the streaming data to the console (for testing)\n",
    "user_query = geo_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination\n",
    "user_query.awaitTermination()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772dcc67-3c93-48f7-9f04-72416715b7ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr, regexp_extract, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KinesisDataReadAndClean\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Get the AWS access key and secret key from the Spark DataFrame\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').first()['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').first()['Secret access key']\n",
    "\n",
    "# Define AWS region and Kinesis stream details\n",
    "kinesis_region = \"us-east-1\"\n",
    "kinesis_stream_name_pin = \"streaming-0eaa2e755d1f-pin\"\n",
    "kinesis_stream_name_geo = \"streaming-0eaa2e755d1f-geo\"\n",
    "kinesis_stream_name_user = \"streaming-0eaa2e755d1f-user\"\n",
    "\n",
    "# Define schemas for JSON data\n",
    "pin_schema = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "geo_schema = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "user_schema = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the cleaning function for pin data\n",
    "def clean_df_pin(df):\n",
    "    # Parse JSON data\n",
    "    df = df.withColumn(\"parsed_json\", from_json(col(\"json_data\"), pin_schema))\n",
    "    df = df.select(\"parsed_json.*\")\n",
    "    \n",
    "    # Clean the follower_count to ensure it's a number and convert to int\n",
    "    df = df.withColumn(\"follower_count\", \n",
    "                       when(col(\"follower_count\").rlike(\"^[0-9]+$\"), col(\"follower_count\"))\n",
    "                       .when(col(\"follower_count\").rlike(\"^[0-9]+[kmKM]+$\"), \n",
    "                             expr(\"CASE WHEN lower(follower_count) LIKE '%k%' THEN int(follower_count * 1000) \" +\n",
    "                                  \"WHEN lower(follower_count) LIKE '%m%' THEN int(follower_count * 1000000) \" +\n",
    "                                  \"ELSE int(follower_count) END\"))\n",
    "                       .cast(\"int\"))\n",
    "    \n",
    "    # Clean the save_location column\n",
    "    df = df.withColumn(\"save_location\", regexp_extract(col(\"save_location\"), r'[^/]+$', 0))\n",
    "    \n",
    "    # Reorder the DataFrame columns\n",
    "    df = df.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \n",
    "                   \"poster_name\", \"tag_list\", \"is_image_or_video\", \n",
    "                   \"image_src\", \"save_location\", \"category\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define the cleaning function for geo data\n",
    "def clean_df_geo(df):\n",
    "    # Parse JSON data\n",
    "    df = df.withColumn(\"parsed_json\", from_json(col(\"json_data\"), geo_schema))\n",
    "    df = df.select(\"parsed_json.*\")\n",
    "    \n",
    "    # Create coordinates column\n",
    "    df = df.withColumn(\"coordinates\", expr(\"array(latitude, longitude)\"))\n",
    "    \n",
    "    # Drop latitude and longitude columns\n",
    "    df = df.drop(\"latitude\", \"longitude\")\n",
    "    \n",
    "    # Convert timestamp to timestamp type\n",
    "    df = df.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "    \n",
    "    # Reorder the DataFrame columns\n",
    "    df = df.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define the cleaning function for user data\n",
    "def clean_df_user(df):\n",
    "    # Parse JSON data\n",
    "    df = df.withColumn(\"parsed_json\", from_json(col(\"json_data\"), user_schema))\n",
    "    df = df.select(\"parsed_json.*\")\n",
    "    \n",
    "    # Create user_name column\n",
    "    df = df.withColumn(\"user_name\", col(\"first_name\") + \" \" + col(\"last_name\"))\n",
    "    \n",
    "    # Drop first_name and last_name columns\n",
    "    df = df.drop(\"first_name\", \"last_name\")\n",
    "    \n",
    "    # Convert date_joined to timestamp type\n",
    "    df = df.withColumn(\"date_joined\", col(\"date_joined\").cast(TimestampType()))\n",
    "    \n",
    "    # Reorder the DataFrame columns\n",
    "    df = df.select(\"ind\", \"user_name\", \"age\", \"date_joined\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Read, clean, and process the pin Kinesis stream\n",
    "pin_kinesis_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", kinesis_stream_name_pin) \\\n",
    "    .option(\"region\", kinesis_region) \\\n",
    "    .option(\"awsAccessKey\", ACCESS_KEY) \\\n",
    "    .option(\"awsSecretKey\", SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "pin_df = pin_kinesis_df \\\n",
    "    .selectExpr(\"CAST(data AS STRING) as json_data\")\n",
    "\n",
    "# Apply cleaning function\n",
    "cleaned_pin_df = clean_df_pin(pin_df)\n",
    "\n",
    "# Write the cleaned streaming data to Delta Table\n",
    "pin_query = cleaned_pin_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/tmp/checkpoints/pin\") \\\n",
    "    .option(\"path\", \"dbfs:/user/hive/warehouse/0eaa2e755d1f_pin_table\") \\\n",
    "    .start()\n",
    "\n",
    "# Read, clean, and process the geo Kinesis stream\n",
    "geo_kinesis_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", kinesis_stream_name_geo) \\\n",
    "    .option(\"region\", kinesis_region) \\\n",
    "    .option(\"awsAccessKey\", ACCESS_KEY) \\\n",
    "    .option(\"awsSecretKey\", SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "geo_df = geo_kinesis_df \\\n",
    "    .selectExpr(\"CAST(data AS STRING) as json_data\")\n",
    "\n",
    "# Apply cleaning function\n",
    "cleaned_geo_df = clean_df_geo(geo_df)\n",
    "\n",
    "# Write the cleaned streaming data to Delta Table\n",
    "geo_query = cleaned_geo_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/tmp/checkpoints/geo\") \\\n",
    "    .option(\"path\", \"dbfs:/user/hive/warehouse/0eaa2e755d1f_geo_table\") \\\n",
    "    .start()\n",
    "\n",
    "# Read, clean, and process the user Kinesis stream\n",
    "user_kinesis_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", kinesis_stream_name_user) \\\n",
    "    .option(\"region\", kinesis_region) \\\n",
    "    .option(\"awsAccessKey\", ACCESS_KEY) \\\n",
    "    .option(\"awsSecretKey\", SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "user_df = user_kinesis_df \\\n",
    "    .selectExpr(\"CAST(data AS STRING) as json_data\")\n",
    "\n",
    "# Apply cleaning function\n",
    "cleaned_user_df = clean_df_user(user_df)\n",
    "\n",
    "# Write the cleaned streaming data to Delta Table\n",
    "user_query = cleaned_user_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/tmp/checkpoints/user\") \\\n",
    "    .option(\"path\", \"dbfs:/user/hive/warehouse/0eaa2e755d1f_user_table\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination of all queries\n",
    "pin_query.awaitTermination()\n",
    "geo_query.awaitTermination()\n",
    "user_query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Kinesis_streaming_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
